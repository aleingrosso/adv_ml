{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083f73ba-d689-42c6-8125-add9171bdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "from scipy import optimize\n",
    "from utils import color_cycle\n",
    "from perceptron_code import train_perceptron, generate_teacher_and_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def05665-a4bc-4ad6-945d-8573604baa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "def plot_points(X, t, ax, lim = 9, alpha = 1):\n",
    "    ax.plot(X[t==1,0], X[t==1,1], 'x', color=\"black\", alpha=alpha);\n",
    "    ax.plot(X[t==0,0], X[t==0,1], 's', color=\"black\", alpha=alpha);\n",
    "    ax.hlines(y=0, xmin=-lim, xmax=lim, ls=':', color='gray')\n",
    "    ax.vlines(x=0, ymin=-lim, ymax=lim, ls=':', color='gray')\n",
    "    plt.gca().set_aspect('equal');\n",
    "\n",
    "\n",
    "def plot_vec(w, fact=1, wlim = 10, label=None, color=None, alpha=0.5):\n",
    "    plt.plot([0, w[0]*fact], [0, w[1]*fact], ls='--', alpha=alpha, color=color, label=label)\n",
    "    plt.plot([-w[1]*fact, w[1]*fact], [w[0]*fact, -w[0]*fact], alpha=alpha, color=color);\n",
    "\n",
    "\n",
    "def get_wgrid(wlim = 10, num = 100):\n",
    "    wrange = np.linspace(-wlim, wlim, num)\n",
    "    w1grid, w2grid = np.meshgrid(wrange, wrange)\n",
    "    wgrid_stacked = np.dstack([w1grid, w2grid])\n",
    "    return w1grid, w2grid, wgrid_stacked\n",
    "\n",
    "\n",
    "def compute_output(w, X):\n",
    "    h = np.dot(w, X.T)\n",
    "    y = 1. / (1 + np.exp(-h))\n",
    "    return y\n",
    "\n",
    "\n",
    "def compute_E(w, alpha, X, t, eps=1e-10):\n",
    "    y = compute_output(w, X)\n",
    "    err = -(t * np.log(y + eps) + (1 - t) * np.log(1 - y + eps)).sum(-1)\n",
    "    reg = 0.5 * alpha * (w**2).sum(-1)\n",
    "    E = err + reg\n",
    "    return err, reg, E\n",
    "\n",
    "\n",
    "def compute_gradE(w, alpha, X, t):\n",
    "    y = compute_output(w, X)\n",
    "    gradE = ((y - t)[:,None] * X).sum(0) + alpha * w\n",
    "    return gradE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15354a67-e0e8-4cdc-a4cd-5b0368fcab0c",
   "metadata": {},
   "source": [
    "# Where are we going with the first part of this course\n",
    "\n",
    "MacKay, first part of **Chapter 41**.\n",
    "\n",
    "_Recommended readings and watching_:\n",
    "\n",
    "* [Understanding deep learning is also a job for physicists](https://www.nature.com/articles/s41567-020-0929-2) by [Lenka Zdeborová](https://people.epfl.ch/lenka.zdeborova/?lang=en)\n",
    "* [A new view on independence](https://projecteuclid.org/journals/annals-of-probability/volume-24/issue-1/A-new-look-at-independence/10.1214/aop/1042644705.full) by [Michel Talagrand](https://en.wikipedia.org/wiki/Michel_Talagrand)\n",
    "* A free online course on [High Dimensional Probability](https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html) by [Roman Vershynin](https://www.math.uci.edu/~rvershyn/)\n",
    "* A free online course on [High Dimensional Analysis: Random Matrices and Machine Learning](https://www.youtube.com/playlist?list=PLY11JnnnTUCabY4nc0hKptrd5qEWtLoo2) by [Roland Speicher](https://www.uni-saarland.de/lehrstuhl/speicher/roland-speicher.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9f2f0d4-f1c0-45bd-9cee-e07becf12936",
   "metadata": {},
   "source": [
    "# Learning as an inference problem\n",
    "\n",
    "**Data set**:\n",
    "$$D=\\{x^\\mu,t^\\mu\\}$$ \n",
    "\n",
    "with $t^\\mu=\\pm 1$ and $\\mu=1,\\ldots,P$.\n",
    "\n",
    "**Conditional model** (probability of label $t^\\mu$ given $x^\\mu$):\n",
    "$$p(t^\\mu|x^\\mu,w)=\\sigma(t^\\mu w\\cdot x^\\mu)$$\n",
    "\n",
    "**Likelihood**:\n",
    "$$p(D|w)=\\prod_\\mu p(t^\\mu|x^\\mu,w)=e^{-\\mathcal{E}(w)}\\qquad \\mathcal{E}(w)=-\\sum_\\mu \\log p(t^\\mu|x^\\mu,w)$$\n",
    "\n",
    "**Prior**:\n",
    "$$p(w)=\\frac{e^{-R(w,\\alpha)}}{Z_w(\\alpha)}\\qquad R(w,\\alpha)=\\alpha \\sum_i w_i^2$$\n",
    "\n",
    "**Posterior**:\n",
    "$$p(w|D)=\\frac{p(D|w)p(w)}{p(D)} \\propto e^{-L(w)}$$\n",
    "with\n",
    "$$L(w)=\\mathcal{E}(w)+ R(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bd000-5b30-410b-a609-793a41dbfefc",
   "metadata": {},
   "source": [
    "# ML versus Bayesian approach\n",
    "\n",
    "In neural network learning one typically computes the **maximum likelihood** or **maximum posterior** solution.\n",
    "\n",
    "For new test point $x^\\text{test}$, using an ML approach:\n",
    "$$D \\rightarrow w_\\mathsf{ml}$$\n",
    "$$p(t|x^\\text{test},w_\\mathsf{ml})$$\n",
    "\n",
    "Bayesian approach requires integration over multiple solutions:\n",
    "$$D \\rightarrow p(w|D)$$\n",
    "$$p(t|x^\\text{test})=\\int dw p(w|D) p(t|x^\\text{test},w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c10ca-3bb5-4ed5-945a-92964c2ba2e6",
   "metadata": {},
   "source": [
    "#### Let's start with an example in low-dimension: a two-dimensional logistic regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b375a4-69d7-4282-bff0-e9a7e7ee8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "Ps = [2, 4, 6]\n",
    "num_P = len(Ps)\n",
    "use_mackay_inputs = True\n",
    "\n",
    "# precompute weight grid\n",
    "wlim = 9\n",
    "w1grid, w2grid, wgrid_stacked = get_wgrid(wlim=wlim, num=100)\n",
    "\n",
    "# generate dataset\n",
    "if use_mackay_inputs:\n",
    "    # reproduction of Mackay Fig. 41.1\n",
    "    Xall = np.array([[5., 5.],\n",
    "                     [-4., -2.],\n",
    "                     [1.5, 1.5],\n",
    "                     [-1, 1.],\n",
    "                     [1.5, 3.5],\n",
    "                     [-9., 0.]])\n",
    "    tall = np.array([1., 0., 0., 1., 1., 0])\n",
    "else:\n",
    "    # random inputs\n",
    "    Xall = np.random.randn(Ps[-1], dim)\n",
    "    tall = np.random.randint(2, size=Ps[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7dd61c-371e-42a9-acaa-e02c644be4d9",
   "metadata": {},
   "source": [
    "#### Let's train on subsets of the problem using GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89210f4-b5a8-4fc2-bafa-f2f2daf8cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "num_iter_gd = 10000\n",
    "\n",
    "# set a common initial condition\n",
    "w0 = np.array([5.,5.])\n",
    "\n",
    "# allocate records\n",
    "ws_gd = np.zeros((num_P, num_iter_gd, 2))\n",
    "errs_gd = np.zeros((num_P, num_iter_gd))\n",
    "regs_gd = np.zeros((num_P, num_iter_gd))\n",
    "Es_gd = np.zeros((num_P, num_iter_gd))\n",
    "\n",
    "for iP, P in enumerate(Ps):\n",
    "    \n",
    "    # select inputs\n",
    "    X = Xall[:P]\n",
    "    t = tall[:P]\n",
    "    \n",
    "    w = w0.copy()\n",
    "    for it in range(num_iter_gd):\n",
    "        err, reg, E = compute_E(w, alpha, X, t)\n",
    "        ws_gd[iP,it] = w.copy()\n",
    "        errs_gd[iP,it] = err\n",
    "        regs_gd[iP,it] = reg\n",
    "        Es_gd[iP,it] = err + alpha * reg\n",
    "        gradE = compute_gradE(w, alpha, X, t)\n",
    "        w -= eta * gradE\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b802827-cddb-4e35-9661-54f8c0037f76",
   "metadata": {},
   "source": [
    "#### Let's now represent the entire posterior over weights by discretizing the weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f15bd-325d-4fd6-b583-7622bcce948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fig = 1\n",
    "fig = plt.figure(figsize=(10, 2 * len(Ps)))\n",
    "for iP, P in enumerate(Ps):\n",
    "    \n",
    "    # select inputs\n",
    "    X = Xall[:P]\n",
    "    t = tall[:P]\n",
    "\n",
    "    # plot inputs\n",
    "    ax1 = fig.add_subplot(len(Ps), 4, count_fig)\n",
    "    plot_points(X, t, ax1)\n",
    "    count_fig += 1\n",
    "\n",
    "    # compute likelihood and un-normalized posterior\n",
    "    errgrid, reggreid, Egrid = compute_E(wgrid_stacked, alpha, X, t)\n",
    "    likwgrid = np.exp(-errgrid)\n",
    "    pwgrid = np.exp(-Egrid)\n",
    "    \n",
    "    # plot likelihood\n",
    "    ax2 = fig.add_subplot(len(Ps), 4, count_fig, projection='3d')\n",
    "    ax2.plot_surface(w1grid, w2grid, likwgrid, edgecolor='royalblue', lw=0.5, rstride=8, cstride=8, alpha=0.4);\n",
    "    count_fig += 1\n",
    "\n",
    "    # plot posterior\n",
    "    ax3 = fig.add_subplot(len(Ps), 4, count_fig, projection='3d')\n",
    "    ax3.plot_surface(w1grid, w2grid, pwgrid, edgecolor='royalblue', lw=0.5, rstride=8, cstride=8, alpha=0.4);\n",
    "    count_fig += 1\n",
    "\n",
    "    # plot countour posterior\n",
    "    ax4 = fig.add_subplot(len(Ps), 4, count_fig)\n",
    "    ax4.imshow(pwgrid, origin=\"lower\", extent=[-wlim,wlim,-wlim,wlim], alpha=0.5)\n",
    "    ax4.contour(w1grid, w2grid, pwgrid);\n",
    "    plt.plot(ws_gd[iP,0,0], ws_gd[iP,0,1], '.', c='black')\n",
    "    plt.plot(ws_gd[iP,:,0], ws_gd[iP,:,1], c='black')\n",
    "    plt.plot(ws_gd[iP,-1,0], ws_gd[iP,-1,1], 'x', c='black')\n",
    "    count_fig += 1\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5786386-a922-474c-95f5-1eb7fca09cb6",
   "metadata": {},
   "source": [
    "#### Compare the ML predition with the Bayesian one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04936d62-9adb-453b-8efb-7318b1071804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([-2.,2.])\n",
    "t_test = +1\n",
    "\n",
    "plot_points(X, t, plt)\n",
    "plt.plot(x_test[0], x_test[1], 'x', c='red');\n",
    "\n",
    "# compute MP output\n",
    "y_gd = compute_output(ws_gd[-1,-1], x_test)\n",
    "\n",
    "# approximate posterior with a grid (expected to work well for large P)\n",
    "all_outs = compute_output(wgrid_stacked, x_test)\n",
    "pwgrid_normalized = pwgrid / pwgrid.sum()\n",
    "y_bayes = (all_outs * pwgrid_normalized).sum()\n",
    "\n",
    "print(\"p(t = +1| x_test, w_MP) =\", y_gd)\n",
    "print(\"p(t = +1| x_test) =\", y_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5180839-9225-4ce7-8cb4-5b51e48423cf",
   "metadata": {},
   "source": [
    "# Question:\n",
    "\n",
    "* How do we compute the integral over the two-dimensional posterior\n",
    "$$p(t|x^\\text{test})=\\int dw p(w|D) p(t|x^\\text{test},w)$$\n",
    "  in order to get the best predictor for a test input?\n",
    "\n",
    "This is particularly important in high dimension. We will use both **Monte Carlo** and **Variational approximations** in the next lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084942aa-166d-4e4f-b482-c91d44245092",
   "metadata": {},
   "source": [
    "# But are high dimensions always bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229e606-a71c-4d81-aca0-68cef57630a9",
   "metadata": {},
   "source": [
    "### Self-averaging / Concentration: the bliss of high dimension\n",
    "\n",
    "**Self-averaging**: *Well-behaved* observables in learning and optimization have negligible fluctuations across random realizations of problem instances\n",
    "\n",
    "**Concentration**: *Well-behaved* functions in high dimensions are almost anywhere constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3ee08-0372-4946-a5e8-2e7638ad9cbd",
   "metadata": {},
   "source": [
    "### An example: computing the average generalization error of a perceptron in a Teacher-student scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a2848-d906-41b5-9375-c10e833bdbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate teacher vector and test patterns\n",
    "N = 200\n",
    "Ptest = 10000\n",
    "T, Xtest, ytest_sign = generate_teacher_and_test_set(Ptest, N)\n",
    "\n",
    "# set parameters\n",
    "lr = 0.1\n",
    "num_epochs = 1000\n",
    "print_every = 1e10\n",
    "parallel = True\n",
    "renormalize = True\n",
    "verbose = True\n",
    "learn_bias = False\n",
    "\n",
    "α_span = np.arange(1, 20, 1)\n",
    "w_span = np.zeros((len(α_span), N))\n",
    "normw_span = np.zeros(len(α_span))\n",
    "err_span = np.zeros(len(α_span))\n",
    "ov_span = np.zeros(len(α_span))\n",
    "gen_err_span = np.zeros(len(α_span))\n",
    "\n",
    "for iα, α in enumerate(α_span):\n",
    "    if verbose:\n",
    "        print(f\"doing α: {α}\")\n",
    "    \n",
    "    P = int(α * N)\n",
    "    X = np.random.randn(P, N)\n",
    "    y = 1. * (X @ T > 0)\n",
    "\n",
    "    w, err, errs, ep = train_perceptron(X, y, learn_bias=learn_bias, lr=lr, num_epochs=num_epochs,\n",
    "                                        print_every=print_every, parallel=parallel, renormalize=renormalize)\n",
    "    w_span[iα] = w\n",
    "    normw_span[iα] = (w**2).sum()\n",
    "    err_span[iα] = err\n",
    "    ov_span[iα] = w @ T / N\n",
    "    gen_err_span[iα] = 1 - (np.sign(Xtest @ w) * ytest_sign > 0).sum() / Ptest\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f47ecca-522d-4c02-a290-4f94c397b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the theory that was revealed to you in a dream\n",
    "res = np.loadtxt(\"theory_perceptron_intro.txt\", delimiter=\",\")\n",
    "αs = res[0]\n",
    "Rs = res[1]\n",
    "gen_errors = res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bebc6-200b-4b49-aa8d-d1fdb1f0f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the theory and experimental points\n",
    "plt.plot(α_span, ov_span, '.', color='blue', label=\"$W \\cdot T$ / N\")\n",
    "plt.plot(αs, Rs, '--', color='blue', label=\"$W \\cdot T$ / N theory\");\n",
    "\n",
    "plt.plot(α_span, gen_err_span, '.', color='red', label=\"gen error\");\n",
    "plt.plot(αs, gen_errors, color='red', label=\"gen error theory\");\n",
    "\n",
    "plt.legend();\n",
    "plt.xlabel('α');\n",
    "plt.ylabel('overlap / gen error');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bd6aef5-c202-44b4-8caf-193a6b22193a",
   "metadata": {},
   "source": [
    "#### By the end-of-year break you will be able to compute the analytical curve and put experimental points on top of it\n",
    "\n",
    "...even in those cases where no gradient-based methods exist to find the correct solution to a problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
