{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1baf9d1-e70c-48e8-9f94-683d7afa12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "from scipy.stats import norm, gamma, cauchy, multivariate_normal, chi2\n",
    "from scipy.special import ndtr, comb\n",
    "\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cbaac-ce03-4350-9a5e-58ad00278360",
   "metadata": {},
   "source": [
    "# Lecture 1: Sampling\n",
    "\n",
    "  * The problem with uniform sampling\n",
    "  * Importance sampling\n",
    "  * Rejection sampling\n",
    "\n",
    "MacKay, **Chapter 29**.\n",
    "\n",
    "_Recommended readings_:\n",
    "\n",
    "* **Chapter 27** of the book [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage) by David Barber\n",
    "* **Chapter 4** of MacKay for more on the typical set and source coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe81f-566f-4f06-94c4-77891c155199",
   "metadata": {},
   "source": [
    "# The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df58a9-0ada-4406-8d91-a930a0650fb6",
   "metadata": {},
   "source": [
    "In many machine learning problems one needs to estimate high dimensional integrals:\n",
    "\n",
    "1.  Estimate expected parameter values in the posterior of a Bayesian learning problem:\n",
    "    $$p(\\theta|D)=\\frac{p(D|\\theta) p(\\theta)}{p(D)}\\qquad \\mathbb{E}\\theta =\\int d\\theta \\theta p(\\theta|D)$$\n",
    "\n",
    "2.  Estimate the *evidence* for model selection\n",
    "    $$p(D|\\mathcal{H}_i)=\\int d\\theta p(D|\\theta,\\mathcal{H}_i) p(\\theta|\\mathcal{H}_i)$$\n",
    "\n",
    "3.  Computing statistics in graphical models $p(x_1,\\ldots,x_n)=\\prod_{i=1}^n p_i(x_i|x_{\\text{Pa}(i)})$\n",
    "    $$p(x_1,x_2)=\\sum_{x_3,\\ldots,x_n} p(x_1,\\ldots,x_n)$$\n",
    "\n",
    "Denote $p(x)=\\frac{p^*(x)}{Z}$ the probability distribution of interest with $Z=\\int dx p^*(x)$.\n",
    "We can assume that $p^*(x)$ is easily evaluated for any $x$, but $Z$ is hard to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5ce56-aca6-49f3-9b6b-d0bcf2caf8e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The problem with uniform sampling\n",
    "\n",
    "Suppose we're only allowed to produce $N$ points $x^r$ in the support of $p^*$. The best way to choose them is to extract them with probability $p(x)=p^*(x)/Z$:\n",
    "$$\\hat{\\Phi}=\\frac{1}{N}\\sum_{r=1}^N \\phi(x^r)\\qquad x^r \\sim p(x)$$\n",
    "$\\hat{\\Phi}$ is a random variable (each time we generate $N$ samples we get a different outcome). If we draw the $x^r$ independently we have\n",
    "\n",
    "$$\\mathbb{E} \\hat{\\Phi}=\\frac{1}{N}\\sum_{r=1}^N\\sum_{x^r} p(x^r)\\phi(x^r)=\\Phi$$\n",
    "\n",
    "The estimator $\\hat{\\Phi}$ is **unbiased**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315becf-2e34-4a1f-ae5c-ab0cfcc8ad80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The variance in $\\hat{\\Phi}$ decreases with the number of samples $N$. Indeed, for a sum of independent random variables $X=\\sum_i X_i$ is a sum of independent random variables, the variance of $X$ is the sum of the variances of each $X_i$. Therefore:\n",
    "\n",
    "$$\\mathrm{Var} \\left[\\hat{\\Phi}\\right] = N \\mathrm{Var} \\left[\\frac{\\phi}{N}\\right] =\\frac{1}{N}\\mathrm{Var} \\left[\\phi\\right] \\qquad \\mathrm{Var} \\left[\\phi\\right] = \\sum_x p(x) (\\phi(x) - \\Phi)^2$$\n",
    "\n",
    "Thus,\n",
    "$$\\hat{\\Phi}=\\Phi + \\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$$\n",
    "\n",
    "The accuracy does not depend on the dimension of the problem. But sampling from $p$ is very hard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faf3be-7958-4974-b44d-8eee5dada02e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Entropy and the typical set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38da18f-0d45-4e53-a6c6-ae90fd5ab289",
   "metadata": {},
   "source": [
    "#### A simple experiment with $N$ coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c75ff2-7193-49ec-858b-e52be6cf9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toss N coins with increasing N\n",
    "num_n = 20\n",
    "num_samples = 100\n",
    "\n",
    "ns = np.logspace(1, 6, num_n, dtype=int)\n",
    "num_heads = np.zeros((num_n, num_samples))\n",
    "\n",
    "for i, n in enumerate(ns):\n",
    "    draws = 1 * (np.random.rand(num_samples, n) <= 0.5)\n",
    "    num_heads[i] = draws.sum(1) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197da9b-988d-4d8d-b334-d117f64659f9",
   "metadata": {},
   "source": [
    "The following plot shows a well known result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f496b3-d75d-4a12-a185-033ad6ac589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(ns, y=num_heads.mean(1), yerr=num_heads.std(1));\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('# of heads');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df22bd-4f20-40ff-bdda-7cc7f86c6230",
   "metadata": {},
   "source": [
    "Let's try to sample from a non-uniform distribution using a uniform proposal. The simplest non-uniform distribution for discrete variables is a set of bernoulli distributed events with $p\\neq0.5$. Let us take $p=0.2$:\n",
    "\n",
    "$$p\\left(x\\right)=\\prod_{i}p^{x_{i}}\\left(1-p\\right)^{1-x_{i}}=e^{\\sum_{i}\\left(x_{i}\\log p+\\left(1-x_{i}\\right)\\log\\left(1-p\\right)\\right)}$$\n",
    "\n",
    "and use\n",
    "\n",
    "$$\n",
    "\\hat{\\Phi}=\\frac{\\sum_{r=1}^{N}\\phi\\left(x^{r}\\right)p\\left(x^{r}\\right)}{\\sum_{r=1}^{N}p\\left(x^{r}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7472ce8-0993-4b0b-86f2-544e9f4a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toss n coins with increasing n\n",
    "p = 0.2\n",
    "num_samples = 10000\n",
    "\n",
    "ns = [5, 10, 20, 50, 100, 500, 1000]\n",
    "num_n = len(ns)\n",
    "num_heads_av = np.zeros(num_n)\n",
    "\n",
    "for i, n in enumerate(ns):\n",
    "    draws = 1 * (np.random.rand(num_samples, n) <= 0.5)\n",
    "    logpx = (draws * np.log2(p) + (1-draws) * np.log2(1-p)).sum(1)\n",
    "    px = 2**logpx\n",
    "    num_heads_av[i] = (draws.sum(1) / n * px).sum() / px.sum()\n",
    "\n",
    "plt.plot(ns, num_heads_av, '.-');\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928fc18-120f-42d3-9899-17b5b8c59a3d",
   "metadata": {},
   "source": [
    "Our interpretation is the following: of all the $2^N$ configurations of outcomes, the ones where the total sum of heads is $N/2$ dominate (in fact they **exponentially dominate**) the counting over a uniform distribution, this being more and more evident as $N$ grows. Indeed, the number of heads have a binomial distribution with mean $Np$ and variance $Np(1-p)$: the relative scaling of the standard deviation wrt the mean is an indication of concentration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f4b29-38cf-4956-97ea-d9af95ac3ac6",
   "metadata": {},
   "source": [
    "#### A slightly more involved example using the Ising model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c451af-7d8d-4656-a372-5dcda83e3c42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Consider the Ising model\n",
    "$$p(x)=\\frac{\\exp\\left(\\beta \\sum_{i>j} x_i x_j w_{ij}\\right)}{Z}=\\frac{p^*(x)}{Z}\\qquad Z=\\sum_xp^*(x)$$\n",
    "with $x=(x_1,\\ldots,x_n)$ and $x_i=\\pm 1, i=1,\\ldots,n$. Total number of states is $2^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7d0b7-cc49-4aeb-a78d-161b4ca266c2",
   "metadata": {},
   "source": [
    "#### Visualize samples from a 32x32 2d Ising model at two different temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe30f1a-c14a-46a8-99a0-2b85770c6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of an Ising model at T = 10\n",
    "Video(\"ising_size32_32_T10.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d85bf-1bca-44b5-a41c-3aba95949d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of an Ising model at the critical temperature\n",
    "Video(\"ising_size32_32_T2.269.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05f07f-fc72-4fbb-bfea-67692d149854",
   "metadata": {},
   "source": [
    "#### Sampling and entropy\n",
    "\n",
    "Draw a long sequence of $N$ samples from a distribution:\n",
    "$$x^1,x^2, \\ldots, x^N$$The probability of the sequence is\n",
    "$$p(x^1,x^2\\ldots, x^N)=p(x^1)P(x^2)\\ldots p(x^N)= \\prod_x p(x)^{N(x)}$$\n",
    "with $N(x)\\approx p(x) N$ the number of times the value $x$ occurs in the string. Thus we have:\n",
    "$$p(x^1,x^2\\ldots, x^N)\\approx \\prod_x p(x)^{p(x)N} =\\left(2^{- H}\\right)^N$$\n",
    "with $H=-\\sum_x p_x \\log_2 p_x$ the entropy of the distribution. Equivalently\n",
    "$$\\prod_{i}p\\left(x_{i}\\right)=2^{N\\frac{\\sum_{i}\\log p\\left(x_{i}\\right)}{N}}\\approx2^{-NH}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cdd8-544a-4272-b6f1-f413040c62f7",
   "metadata": {},
   "source": [
    "Thus, in the large $N$ limit all **typical** strings have the same probability $2^{-NH}$ and all other **non-typical** strings have probability zero.\n",
    "\n",
    "The formula suggests that *on average* for a single sample, there are typical samples with probability $2^{-H}$ and non-typical samples with probability zero.\n",
    "\n",
    "Denote $T$ the set of typical samples. The number of typical samples is thus $|T|\\approx 2^{H}$.\n",
    "\n",
    "The **typical set** should be compared to the total number of states $2^n$. The volume fraction is\n",
    "$$2^{H-n}$$\n",
    "If we sample states $x$ uniformly at random, the probability to hit an element of the typical set is $2^{H-n}$.\n",
    "Thus, one needs on the order of $N_\\text{min}=2^{n-H}$ samples to hit the typical set once and therefore the\n",
    "number of samples\n",
    "$$N\\gg N_\\text{min}=2^{n-H}$$\n",
    "\n",
    "For $n$ binary spins $0\\le H \\le n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a22807-997e-4ebf-941d-a866c7d4beb3",
   "metadata": {},
   "source": [
    "#### Typical set in coin flipping\n",
    "\n",
    "Let us look at a histogram of $-\\log(p(x))$ with $x$ a sequence of $N$ independent tosses of a biased coin with $p=0.1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f220d-6d19-4a9e-a150-804f032b4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "num_samples = 30\n",
    "\n",
    "p = 0.1\n",
    "draws = 1 * (np.random.rand(num_samples, N) <= p)\n",
    "logpx = (draws * np.log2(p) + (1-draws) * np.log2(1-p)).sum(1)\n",
    "\n",
    "H = -N * (p * np.log2(p) + (1-p) * np.log2(1-p))\n",
    "\n",
    "logp_all1 = N * np.log2(p)\n",
    "logp_all0 = N * np.log2(1-p)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(draws, cmap='gray');\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(-logpx);\n",
    "plt.vlines(x=H, ymin=0, ymax=10, ls='--', color='black', label='H');\n",
    "plt.vlines(x=-logp_all1, ymin=0, ymax=10, ls='--', color='red', label='-logp all 1');\n",
    "plt.vlines(x=-logp_all0, ymin=0, ymax=10, ls='--', color='green', label='-logp all 0');\n",
    "plt.xscale('log')\n",
    "plt.legend();\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887c55b-6b11-4649-bca9-5bcebbe2cb0b",
   "metadata": {},
   "source": [
    "#### Typical set in the Ising model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaefe0a-bbf6-4af2-93f3-94d5f8c48273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entropy of an 8x8 spin 2d Ising model with couplings all equal to 1\n",
    "\n",
    "load = np.loadtxt(\"T_H_8_8_ising.txt\")\n",
    "Ts, Hs = load[0], load[1]\n",
    "idx_crit = np.where((np.abs(Ts-2.27)<1e-2))[0][0]\n",
    "\n",
    "plt.plot(Ts, Hs, ls='-');\n",
    "plt.hlines(y=np.log(2), xmin=Ts[0], xmax=Ts[-1], ls=':', color=\"gray\");\n",
    "plt.vlines(x=Ts[idx_crit], ymin=0, ymax=Hs[idx_crit], ls='--', color='black')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('H');\n",
    "\n",
    "print(\"H at T critical:\", Hs[idx_crit])\n",
    "print(\"H at T = inf:\", np.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e64036-2534-4e78-85eb-8992a1774ddb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For (very) high temperature (low $\\beta$) $H \\approx n$ and $N_\\text{min}\\approx 1$ and uniform sampling is feasible\n",
    "\n",
    "Around the critical temperature, $H\\approx n/2$ and $N_\\text{min}\\approx 2^{n/2}$. For $n=1000$ spins this is of order $10^{150}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755775f-bb76-492d-998b-bf71a4e78bfe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Conclusion: Uniform sampling only works for uniform distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602b23a-4142-4a06-9a71-7bd743af1af0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Importance sampling\n",
    "\n",
    "Let us write\n",
    "$$\\Phi=\\int dxp\\left(x\\right)\\phi\\left(x\\right)=\\int dxq\\left(x\\right)\\frac{p\\left(x\\right)}{q\\left(x\\right)}\\phi\\left(x\\right)$$\n",
    "\n",
    "and sample from the distribution $q(x)$ that is\n",
    "* better than uniform;\n",
    "* easy to sample from.\n",
    "\n",
    "For instance, a (spherical) Gaussian:\n",
    "$$Q^*(x)\\propto \\exp\\left(-\\frac{1}{2}\\sum_i x_i^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8ca50-0f1b-4295-958a-26c955b83c06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Consider simple 1-d sampling problem. Given $p(x)$, compute\n",
    "$$\\Phi=\\mathrm{Prob}(x<0)=\\int_{-\\infty}^\\infty \\phi(x) p(x)dx$$\n",
    "with $\\phi(x)=1$ if $x\\le0$ and zero otherwise.\n",
    "\n",
    "**Naive method**: generate $N$ samples $X_i\\sim p$\n",
    "$$\\hat{\\Phi}=\\frac{1}{N}\\sum_{i=1}^N \\phi(X_i)$$\n",
    "\n",
    "**Importance sampling**: consider another distribution $q(x)$. Then\n",
    "$$\\Phi=\\mathrm{Prob}(x<0)=\\int_{-\\infty}^\\infty \\phi(x) \\frac{p(x)}{q(x)}q(x) dx$$\n",
    "\n",
    "Generate $N$ samples $X_i\\sim q$\n",
    "$$\\hat{\\Phi}=\\frac{1}{N}\\sum_{i=1}^N \\phi(X_i)\\frac{p(X_i)}{q(X_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c28650-9479-48bb-9cd2-576e16eedd61",
   "metadata": {},
   "source": [
    "#### The best importance sampler\n",
    "\n",
    "Any importance sampler is unbiased by construction, indeed:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\hat{\\Phi}=\\mathbb{E}\\phi\\left(x\\right)\\frac{p\\left(x\\right)}{q\\left(x\\right)}=\\sum_{x}\\frac{q\\left(x\\right)}{q\\left(x\\right)}p\\left(x\\right)\\phi\\left(x\\right)=\\Phi\n",
    "$$\n",
    "\n",
    "As for the variance, we have\n",
    "\n",
    "$$\\text{Var}\\hat{\\Phi}=\\frac{1}{N}\\text{Var}\\left(\\phi\\left(x\\right)\\frac{p\\left(x\\right)}{q\\left(x\\right)}\\right)$$\n",
    "\n",
    "The minimal variance can be attained by making $\\phi\\left(x\\right)\\frac{p\\left(x\\right)}{q\\left(x\\right)}$ constant, i.e. by taking $q\\left(x\\right)\\propto\\phi\\left(x\\right)p\\left(x\\right)$. In such a case, we would only need a single observation. We're left with the problem of choosing the proportionality constant in $q\\left(x\\right)=\\frac{\\phi\\left(x\\right)p\\left(x\\right)}{Z_{q}}$, i.e. normalize $q$, by imposing\n",
    "\n",
    "$$\\sum_{x}\\phi\\left(x\\right)p\\left(x\\right)=Z_{q}$$\n",
    "\n",
    "from which we see that this approach is hopeless. Nonetheless, we can use this idea to construct $q$ distributions that have high probability mass in those regions where $\\phi$ is nonzero - as we did in the previous example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a5446db-3216-4175-a5e5-39a3a4d85d83",
   "metadata": {},
   "source": [
    "#### A simple example of why you would want to use importance sampling\n",
    "\n",
    "Let's compute the probability of a normal random variable $X$ to be greater than a value `thresh` using both the original density $p$ and a broader candidate $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752d30b-ff77-4dba-b8a5-b51e92287698",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 4\n",
    "num_sample = 10000\n",
    "num_trials = 100\n",
    "pthres_1 = np.zeros(num_trials)\n",
    "pthres_4 = np.zeros(num_trials)\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    normal_1 = multivariate_normal(0, 1)\n",
    "    normal_4 = multivariate_normal(0, 4) # try also a shifted Gaussian with (1,4) options\n",
    "    sample_1 = normal_1.rvs(num_sample)\n",
    "    sample_4 = normal_4.rvs(num_sample)\n",
    "    pthres_1[trial] = (sample_1 > thresh).mean()\n",
    "    pthres_4[trial] = (normal_1.pdf(sample_4) / normal_4.pdf(sample_4) * (sample_4 > thresh)).mean()\n",
    "\n",
    "# visualize a single sample\n",
    "xs = np.linspace(-5, 5, 100)\n",
    "plt.hist(sample_1, bins=50, histtype=\"step\", density=True, label='std=1');\n",
    "plt.hist(sample_4, bins=50, histtype=\"step\", density=True, label='std=4');\n",
    "plt.plot(xs, normal_1.pdf(xs), c=color_cycle[0])\n",
    "plt.plot(xs, normal_4.pdf(xs), c=color_cycle[1]);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846ced-0762-4df5-b73b-6b8ce0d9bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pthres_1, label=f'p(X>{thresh})');\n",
    "plt.hist(pthres_4, label=f'p(X>{thresh}) Importance Sampling');\n",
    "plt.xscale('log')\n",
    "plt.vlines(x=(1-ndtr(thresh)), ymin=0, ymax=100, ls=':', color='black', label='true value');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3eded-2f36-4a64-ae22-a4cb2ef8a2d6",
   "metadata": {},
   "source": [
    "#### The problem with rare events\n",
    "\n",
    "Rare events give large contributions. It it thus best to use wide distribution $q$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753f3108-823f-4564-9306-8db2870c2e0a",
   "metadata": {},
   "source": [
    "#### Importance Sampling with non-normalized densities\n",
    "\n",
    "The importance weights $p(x)/q(x)$ assume that we can evaluate $p(x)$. However, most often $p(x)$ can only be easily computed up to a constant:\n",
    "$$p(x)=\\frac{p^*(x)}{Z} \\qquad Z=\\int dx p^*(x)$$\n",
    "\n",
    "We can thus estimate both numerator and denominator by sampling:\n",
    "$$\\Phi=\\int dx p(x) \\phi(x) =\\frac{\\int dx p^*(x) \\phi(x)}{\\int dx p^*(x)}=\\frac{\\int dx q(x)\\frac{p^*(x)}{q(x)} \\phi(x)}{\\int dx q(x) \\frac{p^*(x)}{q(x)}}$$\n",
    "\n",
    "Sample ${x^r}$ from $q(x)$ and compute\n",
    "$$w_r=\\frac{p^*(x^r)}{q(x^r)}\\qquad \\hat{\\Phi}=\\frac{\\sum_r w_r \\phi(x^r)}{\\sum_r w_r}$$\n",
    "\n",
    "The estimate is biased:\n",
    "$$\\mathbb{E} \\hat{\\Phi}=\\mathbb{E}\\left(\\frac{ \\sum_{r=1}^N w_r \\phi(x^r)}{ \\sum_{r=1}^N w_r}\\right) \\ne\\frac{\\mathbb{E} \\sum_{r=1}^N w_r \\phi(x^r)}{\\mathbb{E} \\sum_{r=1}^N w_r}=\\frac{N \\int dx p^*(x) \\phi(x)}{N\\int dx p^*(x)}=\\Phi$$\n",
    "\n",
    "However, for large $N$:\n",
    "$$\\sum_{r=1}^N w_r= N \\mathbb{E} w_r + \\mathcal{O}\\left(\\sqrt{N}\\right)\\approx N \\mathbb{E} w_r$$\n",
    "$$\\mathbb{E} \\hat{\\Phi}\\approx \\mathbb{E}\\left(\\frac{ \\sum_{r=1}^N w_r \\phi(x^r)}{N \\mathbb{E} w_r}\\right)=\\frac{\\mathbb{E} \\sum_{r=1}^N w_r \\phi(x^r)}{N \\int dx p^*(x)}=\\frac{N \\int dx p^*(x) \\phi(x)}{N\\int dx p^*(x)}=\\Phi$$\n",
    "The estimator $\\hat{\\Phi}$ is asymptotically unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2632bd-e86d-45eb-bfe0-88859e271372",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Rejection sampling\n",
    "\n",
    "Choose, $c$ such that for all $x: c Q^*(x) > P^*(x)$\n",
    "\n",
    "  * generate $x$ from $Q^*(x)$\n",
    "  * generate $u$ uniform from $[0,c Q^*(x)]$\n",
    "  * if $u > P^*(x)$ reject $x$, otherwise accept $x$\n",
    "\n",
    "Probability of a sample $x$ is $Q^*(x) \\frac{P^*(x)}{cQ^*(x)}\\propto P^*(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd392a2-0c10-4ad6-9024-2f2f306fcdf1",
   "metadata": {},
   "source": [
    "#### An example of rejection sampling: Gamma distribution\n",
    "\n",
    "Warning: in the following I used `k` for $c$ just for the sake of confusing readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a700c7-f159-4fc4-b3e6-908b7fc16e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 0.4\n",
    "d = 40 # maximal element for sampling\n",
    "\n",
    "xs = np.arange(0, d, 0.1)\n",
    "plt.plot(xs, gamma.pdf(xs, a, scale=1/b), label=\"Gamma\");\n",
    "plt.vlines(x=a/b, ymin=0, ymax=gamma.pdf(a/b, a, scale=1/b), color='gray', ls=':', label=\"mean\");\n",
    "plt.vlines(x=(a-1)/b, ymin=0, ymax=gamma.pdf((a-1)/b, a, scale=1/b), color='gray', ls='--', label=\"mode\");\n",
    "\n",
    "# using plain Cauchy\n",
    "C = (a - 1)/b\n",
    "k = 2 # the choice of k here is not easy!\n",
    "B = k/(np.pi * gamma.pdf((a-1)/b, a, scale=1/b))\n",
    "plt.plot(xs, k/(1 + (xs - C)**2/B**2)/(np.pi*B), label=\"Cauchy\");\n",
    "# plt.plot(xs, k*cauchy.pdf(xs, loc=C, scale=B)) # just a check!\n",
    "\n",
    "# using renormalized Cauchy\n",
    "C = (a - 1)/b\n",
    "B = 4.5\n",
    "Z_renorm = B * (np.arctan((d-C)/B)-np.arctan(-C/B))\n",
    "k_renorm = Z_renorm * gamma.pdf((a-1)/b, a, scale=1/b)\n",
    "plt.plot(xs, k_renorm/(1 + (xs - C)**2/B**2)/Z_renorm, label=\"Cauchy renorm\");\n",
    "# plt.plot(xs, k*cauchy.pdf(xs, loc=C, scale=B)) # just a check!\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71111622-5ad0-4a61-aa2f-4e749c652c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using a renormalized Cauchy\n",
    "num_samples = 5000\n",
    "t_left = np.arctan(-C/B)\n",
    "t_right = np.arctan((d-C)/B)\n",
    "z = C + B * np.tan(np.random.rand(num_samples) * (t_right-t_left) + t_left)\n",
    "\n",
    "pz = gamma.pdf(z, a, scale=1/b)\n",
    "qz = 1/(1 + (z - C)**2/B**2)/Z_renorm\n",
    "# sample homogeneously from 0 to k q(z)\n",
    "u = k_renorm * qz * np.random.rand(num_samples)\n",
    "accept = u <= pz\n",
    "x = z[accept]\n",
    "num_accept = len(x)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.hist(x, bins=50, density=True, histtype=\"step\", color=color_cycle[0]);\n",
    "plt.plot(xs, gamma.pdf(xs, a, scale=1/b), label=\"p\", color=color_cycle[0]);\n",
    "\n",
    "# note that the density of total samples is q(x)\n",
    "plt.hist(z, bins=50, density=True, histtype=\"step\", color=color_cycle[1]);\n",
    "plt.plot(xs, 1/(1 + (xs - C)**2/B**2)/Z_renorm, '--', label=\"q\", color=color_cycle[1]);\n",
    "plt.legend();\n",
    "\n",
    "# k q(x) can be visualized in a scatter plot\n",
    "plt.subplot(122)\n",
    "plt.scatter(z[accept], u[accept], marker='.', s=2, color=color_cycle[0]);\n",
    "plt.scatter(z[~accept], u[~accept], marker='.', s=2, color=\"red\");\n",
    "plt.plot(xs, gamma.pdf(xs, a, scale=1/b), label=\"p\");\n",
    "plt.plot(xs, k_renorm/(1 + (xs - C)**2/B**2)/Z_renorm, '--', label=\"k q\");\n",
    "plt.legend();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6ad03-f37e-4222-b994-4a6206c1c1f6",
   "metadata": {},
   "source": [
    "# Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f70cbf-9be0-4a25-b67e-8c3465b81d0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Importance sampling in high dimensions\n",
    "\n",
    "Importance sampling breaks down in high dimension because of the large variance of the sample weights $w(x)=\\frac{p(x)}{q(x)}$.\n",
    "\n",
    "\n",
    "The variance in $w$ depends on the difference between the distributions $q$ and $p$ [when $p=q$ one obviously has $w(x)=1$, i.e. weights have zero variance].\n",
    "\n",
    "In high dimension, $\\hat{\\Phi}$ will be dominated by large contributions $w(x)\\phi(x)$ induced by small $q(x)$: such contributions has low probability of occuring in the sample, and thus fluctuate wildly across realization of the importance sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6f754-be21-4090-94bb-c1b6ebb260e0",
   "metadata": {},
   "source": [
    "#### Geometry of spheres and Gaussians in high dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ed6e5-df01-44c9-91a2-c442a06dc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2\n",
    "num_sample = 10000\n",
    "\n",
    "Ns = np.array([10, 20, 50, 100, 200, 300])\n",
    "\n",
    "sqmean, sqvar = np.zeros(len(Ns)), np.zeros(len(Ns))\n",
    "\n",
    "plt.figure(figsize=(4, 2 * len(Ns)))\n",
    "count_fig = 0\n",
    "for i, N in enumerate(Ns):\n",
    "    sample = np.random.randn(num_sample, N) * sigma\n",
    "    sqsample = (sample**2).sum(1)\n",
    "    sqmean[i] = sqsample.mean()\n",
    "    sqvar[i] = sqsample.var()\n",
    "\n",
    "    count_fig += 1\n",
    "    plt.subplot(len(Ns), 1, count_fig)\n",
    "    plt.hist(sqsample, bins=50, density=True, alpha=0.5);\n",
    "    mean = N * sigma**2\n",
    "    var = 2 * N * sigma**4\n",
    "    normal = multivariate_normal(mean, var)\n",
    "    chisq = chi2(N, scale=sigma**2)\n",
    "    xs = np.linspace(mean - 3 * np.sqrt(var), mean + 3 * np.sqrt(var))\n",
    "    plt.plot(xs, chisq.pdf(xs), c=color_cycle[0], label='$χ^2$');\n",
    "    plt.plot(xs, normal.pdf(xs), c=color_cycle[1], label='normal');\n",
    "    plt.legend();\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76304881-8eb9-418a-b3dd-3a9248b6b568",
   "metadata": {},
   "source": [
    "The previous plots show that in high dimensions the mass of a multidimensional Gaussian is concentrated in a shell or radius $\\sqrt{N} \\sigma$ [note that such region is pretty far away from the mode of the distribution]. Indee, the quantity $\\rho(x)=\\sqrt{\\sum_i x_i^2}$ has an approximately Gaussian distribution with mean and variance expressed by:\n",
    "\n",
    "$$\\rho^{2}\\sim N\\sigma^{2}\\pm\\sigma^{2}\\sqrt{2N}$$\n",
    "\n",
    "Visualize such relations in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc78072-a779-4ca5-8855-a4ee1e980257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ns, sqmean, '.', c=color_cycle[0]);\n",
    "plt.plot(Ns, Ns * sigma**2, c=color_cycle[0], alpha=0.5, label=\"av $ρ^2$\");\n",
    "plt.plot(Ns, sqvar, '.', c=color_cycle[1]);\n",
    "plt.plot(Ns, Ns * 2 * sigma**4, '-', c=color_cycle[1], alpha=0.5, label=\"var $ρ^2$\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('av $ρ^2$, var $ρ^2$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9684606-67ed-4728-8887-a1079ae24c24",
   "metadata": {},
   "source": [
    "This concentration property extends to a sphere in $N$ dimension. Since the sphere volume scales with the radius $r$ as $r^N$, one has that for a small shell around the radius\n",
    "\n",
    "$$\\frac{r^{N}-\\left(r-\\epsilon\\right)^{N}}{r^{N}}=1-\\left(1-\\frac{\\epsilon}{r}\\right)^{N}$$\n",
    "\n",
    "thus all the volume is concentrated in the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0236a0a-0b88-4b5a-8d0a-f4de5924f027",
   "metadata": {},
   "source": [
    "#### Importance sampling in high dimensions (reprise)\n",
    "\n",
    "Suppose now we may want to sample from a uniform distribution $p(x)=p^*(x)/Z_P$ inside an $N$ dimensional sphere of radius $R_P$ using for $q$ a factorized Gaussian. We have:\n",
    "\n",
    "$$p^{*}\\left(x\\right)=\\theta\\left(R_{P}-\\rho\\left(x\\right)\\right)$$\n",
    "\n",
    "with $\\theta$ the Heaviside function.\n",
    "\n",
    "Suppose we are able to choose $\\sigma$ in such a way that the shell lies inside the $R_P$ sphere. Samples from $q$ will have an approximate probability\n",
    "\n",
    "$$\\frac{1}{\\left(2\\pi\\sigma^{2}\\right)^{\\frac{N}{2}}}\\exp\\left(-\\frac{N}{2}\\pm\\sqrt{\\frac{N}{2}}\\right)$$\n",
    "\n",
    "and thus a typical weight $w(x)=p^*(x)/q(x)$ equal to\n",
    "\n",
    "$$\\left(2\\pi\\sigma^{2}\\right)^{\\frac{N}{2}}\\exp\\left(\\frac{N}{2}\\pm\\sqrt{\\frac{N}{2}}\\right)$$\n",
    "\n",
    "We see that their fluctuations scale like $\\exp(\\sqrt{N})$.\n",
    "\n",
    "In summary: even if we are able to make sure that $q$ generates samples from the typical set of $p$, the resulting weight will have a very large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af881811-894d-4eff-aee4-b7a33ee953d0",
   "metadata": {},
   "source": [
    "#### Rejection sampling in high dimensions\n",
    "\n",
    "Rejection sampling is inefficient in high dimensions. Consider  $p(x)$ and $q(x)$ spherical Gaussians in $n$ dimensions with mean 0 and\n",
    "$\\sigma_q =1.01 \\sigma_p$.\n",
    "\n",
    "Since\n",
    "$$q(x)=\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2_q}}\\right)^ne^{-\\frac{1}{2\\sigma_q^2}\\sum_i x_i^2}\\qquad p(x)=\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2_p}}\\right)^n e^{-\\frac{1}{2\\sigma_p^2}\\sum_i x_i^2}$$then$$c=\\frac{p(0)}{q(0)}=\\left(\\frac{\\sigma_q}{\\sigma_p}\\right)^n=1.01^n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a536-3f4f-41fd-af18-36803a7529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_p = 10\n",
    "sigma_q = 1.01 * sigma_p\n",
    "N = 1000\n",
    "c = (sigma_q/sigma_p)**N\n",
    "print(\"c =\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896cca9-ee22-4abb-b41f-e09de5d21098",
   "metadata": {},
   "source": [
    "Thus volume under $c q$ is 20.000 times the volume under $p$. Therefore, the acceptance rate = $\\frac{\\mbox{volume}~p}{\\mbox{volume}~c q}=\\frac{1}{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1a382-b223-4fc6-90b0-aec4ed05de89",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b33fe3-e9ed-488f-86f9-0bec1249c5f2",
   "metadata": {},
   "source": [
    "#### Ex 1.1\n",
    "\n",
    "Sampling from a one dimensional Gaussian can be done by sampling from a uniform distribution using the **Box-Muller transformation**. \n",
    "\n",
    "Consider two variables $x_1$ and $x_2$ sampled independently from a uniform distribution in $[0,1]$. Take:\n",
    "\n",
    "$$y_{1}=\\sqrt{-2\\log x_{1}}\\cos\\left(2\\pi x_{2}\\right)$$\n",
    "$$y_{2}=\\sqrt{-2\\log x_{1}}\\sin\\left(2\\pi x_{2}\\right)$$\n",
    "\n",
    "Show that $y_1$ and $y_2$ are uncorrelated normal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5631e-26fe-4920-9678-8f601735f1ba",
   "metadata": {},
   "source": [
    "#### Ex 1.2 (MacKay Ex 29.13)\n",
    "\n",
    "This exercise shows that importance sampling can fail even for Gaussian distributions in one dimension.\n",
    "\n",
    "We wish to sample from a zero-mean one-dimensional Gaussian $p$ with variance $\\sigma^2_p$, using a zero-mean Gaussian $q$ with variance $\\sigma^2_q$. Compute the variance of the importance weights. What happens when $\\sigma^2_q=\\sigma^2_p / 2$? Reproduce Figure 29.20 in MacKay's book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
