{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ff01ad-c268-48cc-8279-d6016fe77973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf69bf-34c5-4976-8b29-d6d694c7ee5e",
   "metadata": {},
   "source": [
    "# Lecture 2.2: Markov Chain Monte Carlo\n",
    "\n",
    "* Metropolis-Hastings algorithm\n",
    "* Gibbs sampling\n",
    "* Hybrid Monte Carlo\n",
    "\n",
    "MacKay, **Chapter 29** and **Chapter 30**.\n",
    "\n",
    "_Recommended readings_:\n",
    "\n",
    "* [Timothy Budd](https://hef.ru.nl/~tbudd/) has an excellent course on Monte Carlo techniques - lecture material is available as a Jupyter book [here](https://hef.ru.nl/~tbudd/mct/intro.html).\n",
    "* [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434) by [Michael Betancourt](https://betanalpha.github.io/)\n",
    "* [MCMC using Hamiltonian dynamics](https://arxiv.org/abs/1206.1901) and [Bayesian Learning for Neural Networks](https://glizen.com/radfordneal/ftp/thesis.pdf) by [Radford Neal](https://glizen.com/radfordneal/)\n",
    "* [Stan](https://mc-stan.org/docs/stan-users-guide/index.html) user guide\n",
    "* An informed [post](https://statmodeling.stat.columbia.edu/2025/10/03/its-a-jax-jax-jax-jax-world/#:~:text=Like%20Stan%2C%20JAX%20is%20also,it's%20wonderfully%20compositional%20and%20general.&text=As%20much%20as%20people%20like,to%20just%20write%20JAX%20directly.) on the current state of affairs in Bayesian modeling: [Stan](https://mc-stan.org/) vs [JAX](https://github.com/jax-ml/jax) [vs [PyMC](https://www.pymc.io/welcome.html) vs [NumPyro](https://github.com/pyro-ppl/numpyro)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe3667-c499-4348-91ff-05cab5119b5a",
   "metadata": {},
   "source": [
    "# Metropolis-Hastings algorithm\n",
    "\n",
    "Suppose we would like to sample from a distribution $p(x) = p^*(x)/Z$.\n",
    "\n",
    "The Metropolis-Hastings (MH) algorithm is composed of two steps:\n",
    "* **propose** a state $x'$ using a distribution $q(x'|x^r)$ that generally depends on the current sample $x^r$\n",
    "* **accept** the state $x'$ with probability\n",
    "  $$p_{a}\\left(x',x^r\\right)=\\min\\left(1,a\\left(x',x^r\\right)\\right)$$\n",
    "  $$a\\left(x',x^{r}\\right)=\\frac{p^{*}\\left(x'\\right)q\\left(x^{r}|x'\\right)}{p^{*}\\left(x^{r}\\right)q\\left(x'|x^{r}\\right)}$$\n",
    "\n",
    "  If $x'$ is accepted:\n",
    "  $$x^{r+1}=x'$$\n",
    "  else\n",
    "  $$x^{r+1}=x^r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f5d62-f517-4f3b-a6ef-59578543aa12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Why MH works\n",
    "\n",
    "We would like to prove that MH transition satisfies DB, i.e.\n",
    "\n",
    "\n",
    "Consider two states $x$, $x'$ and note that $a\\left(x',x\\right)=\\frac{1}{a\\left(x,x'\\right)}$\n",
    "\n",
    "Suppose $a_{x'x}\\ge 1$. Then:\n",
    "\n",
    "  * Given $x$ the probability to accept $x'$ is\n",
    "    $$T(x'|x)=q(x'|x)$$\n",
    "  * Given $x'$ the probability to accept $x$ is\n",
    "    $$T(x|x')=q(x|x')a_{xx'}$$\n",
    "    $$\\frac{T(x'|x)}{T(x|x')}=a_{x'x}\\frac{q(x'|x)}{q(x|x')}=\\frac{p^*(x')}{p^*(x)} =\\frac{p(x')}{p(x)},$$\n",
    "\n",
    "The MH algorithm is thus an ergodic (aperiodic) Markov process that satisfies detailed balance: the process $T(x'|x)$ converges to $p(x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "468ce837-6e57-4585-b8a7-e2c5c91ee228",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Gibbs sampling\n",
    "\n",
    "At each time step, update a single variable $i$ using its conditional distribution from the target distribution $p(x)$.\n",
    "\n",
    "$$p(x_i'|x_{\\setminus i})$$\n",
    "\n",
    "keeping all other elements $x'_{\\setminus i}$ unchanged [$x_{\\setminus i}$ is the vector of variables without $x_i$].\n",
    "\n",
    "A Gibbs sampling step is a MH step with acceptance $a=1$. Indeed the update of the $x\\to x'$ is described by\n",
    "$$q(x'|x)=p(x'_i|x_{\\setminus i})\\delta_{x_{\\setminus i},x'_{\\setminus i}}$$\n",
    "and for the acceptance:\n",
    "$$a=\\frac{p(x')}{p(x)}\\frac{q(x|x')}{q(x'|x)}=\\frac{p(x_i'|x'_{\\setminus i})p(x'_{\\setminus i})}{p(x_i|x_{\\setminus i})p(x_{\\setminus i})}\\frac{p(x_i|x'_{\\setminus i})}{p(x'_i|x_{\\setminus i})}=\\frac{p(x_i'|x_{\\setminus i})}{p(x_i|x_{\\setminus i})}\\frac{p(x_i|x_{\\setminus i})}{p(x'_i|x_{\\setminus i})}=1$$\n",
    "where we used $x'_{\\setminus i}=x_{\\setminus i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70b9cc-cd87-4d41-8e72-b6e42df4308e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### *Bonus: MH vs Gibbs in a discrete space*\n",
    "\n",
    "Let us take a distribution of the usual form\n",
    "$$p(x)=\\frac{1}{Z}\\exp \\left(- E(x)\\right)$$\n",
    "with $\\left\\{ -1,1\\right\\} ^{N}$. Let us introduce the flip operator $F_i$, i.e. $F_i x = (x_{\\setminus i}, -x_i)$.\n",
    "\n",
    "Given state $x=(x_{\\setminus i},x_i)$, the probability to go to state $x'=(x_{\\setminus i},-x_i)$ is\n",
    "$$\\text{MH}:~~~p(x'|x)=\\text{min}\\left(e^{-\\Delta E_{x,F_i x}},1\\right)$$\n",
    "$$\\text{Gibbs}:~~~p(x'|x)=p(-x_i|x_{\\setminus i})=\\frac{p(-x_i,x_{\\setminus i})}{p(x_{\\setminus i})}=\\frac{e^{-E(-x_i,x_{\\setminus i})}}{e^{-E(x_i,x_{\\setminus i})}+e^{-E(-x_i,x_{\\setminus i})}}=\\frac{1}{1+e^{\\Delta E_{x,F_i x}}}$$\n",
    "\n",
    "Both transitions respect detailed balance:\n",
    "$$\\text{MH}:~~~\\frac{p(x'|x)}{p(x|x')}=\\frac{\\min\\left(e^{-\\Delta E},1\\right)}{\\min\\left(e^{\\Delta E},1\\right)}=e^{-\\Delta E}=\\frac{p(x')}{p(x)}$$\n",
    "\n",
    "$$\\text{Gibbs}:~~~\\frac{p(x'|x)}{p(x|x')}=\\frac{p(-x_i|x_{\\setminus i})}{p(x_i|x_{\\setminus i})}=\\frac{1+e^{-\\Delta E}}{1+e^{\\Delta E}}=e^{-\\Delta E}=\\frac{p(x')}{p(x)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2096bb26-a6d3-4838-a653-8f016dc83e86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Correlations slow down sampling\n",
    "\n",
    "When $q(x'|x)$ is Gaussian centered on $x$, $\\frac{q(x'|x)}{q(x|x')}$ independent of $x,x'$:\n",
    "$$a_{x'x}=\\frac{p^*(x')}{p^*(x)}$$\n",
    "\n",
    "**$\\epsilon$ large:**\n",
    "Acceptance rate $a_{x'x}$ small.\n",
    "\n",
    "**$\\epsilon$ small:**\n",
    "Strong dependence on starting value.\n",
    "Many samples needed to sample.\n",
    "\n",
    "With step $\\epsilon$ random, the particle moves a distance $L \\approx \\sqrt{T} \\epsilon$ in $T$ iterations. If largest length scale is $L$ then\n",
    "$$T\\approx \\left(\\frac{L}{\\epsilon}\\right)^2$$\n",
    "time steps are needed to sample the length $L$.\n",
    "\n",
    "Convergence slows down when variables are correlated: you will experiment with this problem in **Ex 2.1**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f57e8f1-f5e4-4ab8-b709-e6fd4bb5f2e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Hybrid Monte Carlo\n",
    "\n",
    "### In theory\n",
    "\n",
    "The idea behing Hybrid (or Hamiltonian) Monte Carlo (HMC) is to use the information in $\\nabla_x \\log p(x)$ to reduce the diffusive (random walk) behaviour of the MH method.\n",
    "\n",
    "Consider a distibution $p$ on an $N$-dimensional space\n",
    "$$p(x)=\\frac{e^{-E(x)}}{Z}$$\n",
    "where $E$ and its gradient $\\frac{\\partial E}{\\partial x_i}$ are easy to compute.\n",
    "\n",
    "For each $x_i$, introduce a *momentum* $p_i$ variable, thus doubling the state space to $2N$ variables.\n",
    "\n",
    "Define the *Hamiltonian*\n",
    "$$H(p,x)=E(x)+\\frac{\\alpha}{2}\\sum_i p_i^2$$\n",
    "and a new distribution in the enlarged space\n",
    "$$P_H(p,x)=\\frac{1}{Z_H}\\exp\\left(-E(x)-\\frac{\\alpha}{2}\\sum_i p_i^2\\right)$$\n",
    "\n",
    "Sampling from $P_H$ can be accomplished by using Hamilton equations:\n",
    "$$\\frac{d p_i}{d t}=-\\frac{\\partial H}{\\partial x_i} \\qquad \\frac{d x_i}{d t}=\\frac{\\partial H}{\\partial p_i}$$\n",
    "which leave $H$ invariant:\n",
    "\n",
    "$$\\frac{dH}{dt}=\\sum_i \\frac{dH}{dp_i}\\frac{dp_i}{dt}+\\frac{dH}{dx_i}\\frac{dx_i}{dt}=0$$\n",
    "\n",
    "The original distribution $p$ can be obtained by marginalizing over the momentum variables:\n",
    "$$p(x)=\\int dp P_H(p,x)$$\n",
    "\n",
    "In practice any numerical integration will deviate from the true Hamiltonian flow - even when using *simplectic integrators* like **leapfrog**.\n",
    "\n",
    "We therefore use the resulting configuratin from Hamiltonian dynamics as a proposal and accept it using the MH protocol."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d40459-cc94-42e5-a62f-e0d45a11f4ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Hybrid Monte Carlo\n",
    "\n",
    "### In practice\n",
    "\n",
    "Choose initial point $x^1$.\n",
    "\n",
    "For t up to T\n",
    "\n",
    "1.  choose $p^t$ from $\\mathcal{N}(0,\\alpha^{-1})$, yielding $(x^t,p^t)$\n",
    "2.  run Hamilton dynamics using Leapfrog, yielding $(x',p')$\n",
    "3.  accept $(x^{t+1},p^{t+1})=(x',p')$ as new state using MH:\n",
    "    $$\\mathsf{min}\\left(1,a\\right)\\qquad a=\\frac{P_H(x',p')}{P_H(x^t,p^t)}=\\frac{e^{-H(x',p')}}{e^{-H(x^t,p^t)}}$$\n",
    "4.  On rejection, $(x^{t+1},p^{t+1})=(x^t,p^t)$\n",
    "\n",
    "As said earlier, although the acceptance probability is $a=1$ in theory, it deviates from $1$ due to integration errors.\n",
    "\n",
    "#### Leapfrog\n",
    "\n",
    "Calling \n",
    "$$g_{t}=\\nabla_{x}H\\left(x_{t}\\right)$$\n",
    "the gradient of $H$ wrt the $x$ variables, the update equations for the leapfrog integrators with integration parameter $\\epsilon$ are the following:\n",
    "\n",
    "$$p_{t+\\frac{1}{2}}=p_{t}-\\frac{\\epsilon}{2}g_{t}$$\n",
    "$$x_{t+1}=x_{t}+\\epsilon p_{t+\\frac{1}{2}}$$\n",
    "$$p_{t+1}=p_{t+\\frac{1}{2}}-\\frac{\\epsilon}{2}g_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04bda6-0fcb-4dde-bf80-8b40a42eaad7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# An example of HMC\n",
    "\n",
    "Consider the double well cost $E(x)=\\left(x^2-1\\right)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad18a6-09b3-434b-956d-c569b5d59590",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-2, 2, 100)\n",
    "Es = (xs**2 - 1)**2\n",
    "gradEs = (4 * xs * (xs**2 - 1))\n",
    "plt.plot(xs, Es, label='E', color='black');\n",
    "plt.plot(xs, -gradEs, label='-dE/dx', c='red', alpha=0.5);\n",
    "plt.hlines(y=0, xmin=-2, xmax=2, ls=':', color='gray')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('E, dE/dx')\n",
    "plt.ylim([-10,10])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b41f8-e679-4a9b-a924-4728fd0b0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a Langevin dynamics\n",
    "num_iter = 10000\n",
    "dt = 0.1\n",
    "sigma = 0.5 # try small sigma, like 0.1\n",
    "\n",
    "sqdt = np.sqrt(dt)\n",
    "xstory = np.zeros(num_iter)\n",
    "\n",
    "x = -1.\n",
    "for t in range(num_iter):\n",
    "    xstory[t] = x\n",
    "    x = (1-dt) * x - dt * (4 * x * (x**2 - 1)) + sigma * np.random.randn() * sqdt\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(xstory);\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(xs, Es, label='E');\n",
    "plt.hist(xstory, bins=100, alpha=0.5, density=True);\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2a0b8-3459-49a3-a0cb-d84b4de6820e",
   "metadata": {},
   "source": [
    "The Hamiltonian $H(x,p)=E(x) +\\frac{1}{2} \\alpha p^2$ has the form.\n",
    "\n",
    "The parameter $\\alpha$ plays the roll of inverse inertia: small $\\alpha$ will yield large $p$ values and dynamical trajectories travelling large distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc014f1a-2ad6-49ff-9a0d-b4cbc6981c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "invsqalpha = 1. / np.sqrt(alpha)\n",
    "ps = np.linspace(-2, 2, 100)\n",
    "xgrid, pgrid = np.meshgrid(xs, ps)\n",
    "Hgrid = (xgrid**2 - 1)**2 + 0.5 * alpha * pgrid**2\n",
    "\n",
    "plt.imshow(Hgrid, origin=\"lower\", extent=[-2, 2, -2, 2], alpha=0.8, cmap=\"viridis\");\n",
    "plt.contour(xgrid, pgrid, Hgrid);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p')\n",
    "plt.title('H');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1759df4-edde-437e-be0d-75b61dda691e",
   "metadata": {},
   "source": [
    "Here's an example of four subsequent (accepted) proposals in a HMC with 10 leapfrog steps and integration constant `eps = 0.1`. In the assignment, you will get to implement HMC and observe such trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50981dbc-67f0-4b0c-b9f9-e033f295efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xstories_alpha1 = np.loadtxt(\"data/xstories_alpha1.dat\")\n",
    "pstories_alpha1 = np.loadtxt(\"data/pstories_alpha1.dat\")\n",
    "\n",
    "# visualize_stories\n",
    "for it in range(4):\n",
    "    plt.plot(xstories_alpha1[it,0], pstories_alpha1[it,0], '.', alpha=0.6, c=color_cycle[it]);\n",
    "    plt.plot(xstories_alpha1[it,-1], pstories_alpha1[it,-1], 'x', alpha=0.6, c=color_cycle[it]);\n",
    "    plt.plot(xstories_alpha1[it,:], pstories_alpha1[it,:], alpha=0.6, c=color_cycle[it]);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p');\n",
    "plt.title('H');\n",
    "\n",
    "plt.imshow(Hgrid, origin=\"lower\", extent=[-2, 2, -2, 2], alpha=0.1, cmap=\"viridis\");\n",
    "plt.contour(xgrid, pgrid, Hgrid, alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde01f6-a6bb-4a25-919e-18719ba22f91",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27fd36-3731-48be-b58e-65df6a95a207",
   "metadata": {},
   "source": [
    "#### Ex 2.1\n",
    "\n",
    "In this exercise, you will study the effect of strong correlations between variables on the efficiency of MCMC.\n",
    "\n",
    "Consider the 2-dimensional zero-mean Gaussian distribution with the following covariance matrix:\n",
    "\n",
    "$$\\left(\\begin{array}{cc}\n",
    "1 & 0.998\\\\\n",
    "0.998 & 1\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "1. Write a computer program to sample from this distribution using the Metropolis Hastings algorithm. Study the acceptance ratio and how well the sampler covers the entire distribution by varying the width $\\sigma$ of an isotropic Gaussian proposal distribution. Report the optimal values.\n",
    "2. Write a computer program to sample from this distribution using the Hamilton Monte Carlo algorithm. Study the acceptance ratio and how well the sampler covers the entire distribution by varying the step size $\\epsilon$ and the number of leap frog steps $\\tau$. Report the optimal values.\n",
    "3. Compute the mean of this Gaussian distribution using both methods and compare the accuracy as a function of the computation time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df14c6e7-314a-488e-98c8-81cf350e7ccd",
   "metadata": {},
   "source": [
    "#### Ex 2.2\n",
    "\n",
    "##### Setting\n",
    "\n",
    "Consider the learning problem for logistic regression as explained in **Lecture1.1** and in MacKay chapters **39** and **41**. Our aim is to sample from the distribution $p\\left(w|D, α\\right)$ as given by MacKay Eqs. 41.8-10. Let us recap it in the following (note: the notation is slightly different from the book).\n",
    "\n",
    "**Data set**:\n",
    "$$D=\\{x^\\mu,t^\\mu\\}$$ \n",
    "\n",
    "\n",
    "with $t^\\mu=0, 1$ and $\\mu=1,\\ldots,P$.\n",
    "\n",
    "**Conditional model** (probability of label $t$ given $x$):\n",
    "\n",
    "$$p(t=1|x,w)=\\sigma(w_0+w_1 x_1 +w_2 x_2)$$\n",
    "$$\\sigma(h)=\\frac{1}{1+\\exp(-h)}$$\n",
    "\n",
    "**Likelihood**:\n",
    "$$p(D|w)=\\prod_\\mu p(t^\\mu|x^\\mu,w)=e^{-\\mathcal{E}(w)}\\qquad \\mathcal{E}(w)=-\\sum_\\mu \\log p(t^\\mu|x^\\mu,w)$$\n",
    "\n",
    "**Prior**:\n",
    "$$p(w)=\\frac{e^{-R(w,\\alpha)}}{Z_w(\\alpha)}\\qquad R(w,\\alpha)=\\alpha \\sum_i w_i^2$$\n",
    "\n",
    "**Posterior**:\n",
    "$$p(w|D)=\\frac{p(D|w)p(w)}{p(D)} \\propto e^{-L(w)}$$\n",
    "with\n",
    "$$L(w)=\\mathcal{E}(w)+R(w)$$\n",
    "\n",
    "##### Exercise:\n",
    "\n",
    "Implement MH and Hamiltonian Monte Carlo to sample from the distribution $p\\left(w|D, α\\right)$ (experiment with various values of $\\alpha$. For both methods, reproduce plots similar to MacKay Fig. 41.5 and estimate the burn in time that is required before the sampler reaches the equilibrium distribution.\n",
    "\n",
    "Investigate the acceptance ratio for both methods and try to optimize this by varying the proposal distribution, the step size in HMC and the number of leap frog steps $\\tau$.\n",
    "\n",
    "Use the 10 2-dimensional inputs in `X` and their associated binary targets `t` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2d36a-0bfc-481e-8b07-86f71b495985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset to be used\n",
    "X = np.array([[2., 3.], [3., 2.], [3., 6.], [5.5, 4.5], [5., 3.], [7., 4.], [5., 6.], [8., 6.], [9.5, 5.], [9., 7.]])\n",
    "t = np.array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
    "\n",
    "plot_points(X, t, plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bef11-8eb1-4096-bae6-a306df61666b",
   "metadata": {},
   "source": [
    "#### Ex 2.3\n",
    "\n",
    "Consider the Algorithm 41.4 in MacKay, in the section *The Langevin Monte Carlo method*.\n",
    "\n",
    "Explain why such implementation uses a single leapfrog step, as per the caption: *To obtain the Hamiltonian Monte Carlo method, we repeat the four lines marked * multiple times (algorithm 41.8)*.\n",
    "\n",
    "Discuss the relation with the [Metropolis-adjusted Langevin algorithm](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm) (MALA) and the need of an appropriate choice of `epsilon` for the leapfrog step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
